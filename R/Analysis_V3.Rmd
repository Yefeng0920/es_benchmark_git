---
title: "Effect size benchmark"
header-includes: \usepackage{amsmath}
output:
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 2
    number_sections: yes
    keep_tex: no
---

# Introduction

Our aim is to provide the empirical effect size benchmark using publication-bias adjusted meta-analytic mean effects. This is meaningful, although people would argue that the interpretation of effect size is dependent on the research question at hand.

The existing effect size benchmark has four issues:

(1) Cohen's benchmark is basically based on his intuition and not evidence-based;

(2) Some researchers provide empirical benchmarks, but they do not account for publication bias;

(3) The simple use of absolute value of the effect size overlooks the  distributional properties (e.g., shape and skewness);

(4) When users interpret the magnitude of effect size, they ignore the sampling variance around the estimation.

# Packages

```{r package, warning=FALSE, echo=TRUE}
set.seed(2024)
suppressPackageStartupMessages({
  library(dplyr)
  library(readr)
  library(tidyr) 
  library(here)
  library(ggplot2)
  library(ggpubr)
  library(metafor)
  library(metaSEM)
  library(TOSTER)
  library(cowplot)
  library(car)
  library(patchwork)
  library(RoBMA)}
  )
```

# Function

Function calculating mean and variance based on folded distribution

```{r fun}
## folded mean
folded_es <- function(mean, variance){   
  mu <- mean
  sigma <- sqrt(variance)
  fold_mu <- sigma*sqrt(2/pi)*exp((-mu^2)/(2*sigma^2)) + mu*(1 - 2*pnorm(-mu/sigma))
  fold_mu
}

## folded variance
folded_var <- function(mean, variance){
  mu <- mean
  sigma <- sqrt(variance)
  fold_mu <- sigma*sqrt(2/pi)*exp((-mu^2)/(2*sigma^2)) + mu*(1 - 2*pnorm(-mu/sigma))
  fold_se <- sqrt(mu^2 + sigma^2 - fold_mu^2)
  # variance
  fold_v <- fold_se^2
  fold_v
}


# alternatively
foldnorm <-  function(mu, var){
	   sd <- sqrt(var)
     postfnorm <- stats::dnorm(mu, 0, sd)*2*sd^2 + mu*(2*stats::pnorm(mu, 0, sd) -1)
		    est <- postfnorm
		    var.fnorm <- mu^2 + sd^2 - (sd*sqrt(2/pi)*exp((-1*mu^2)/(2*sd^2)) + mu*(1-2*stats::pnorm(-1*mu/sd, 0, 1)))^2
		    est <- data.frame(Mean=est, Variance = var.fnorm)
}
```

# Data

First, we load the dataset containing meta-analytic level effect sizes and standard errors.

```{r data}
# load meta-analytically derived estimates
df <- read.csv(here("Dat","est.csv"))
names(df)[3:7] <- c("mu_adj", "se_adj", "mu_unadj", "se_unadj", "discipline")
# study info
studies_info <- readRDS(here("Dat","studies_info.RDS"))

# calculate sampling variance of mu
df <- df %>% mutate(var_adj = se_adj^2,
                    var_unadj = se_unadj^2)
# adjust order
df <- df[c(1,2,5,6,9,3,4,8,7)]

# calculate two-sided p values based on standard normal distribution
df <- df %>% mutate(zval = mu_adj / se_adj,
                    pval = pnorm(abs(zval), lower.tail = F) * 2)




# load individual level estimates
datlist_eco <- readRDS(here("Dat/obs","dat_economics.RDS"))
datlist_env <- readRDS(here("Dat/obs","dat_environmental.RDS"))
datlist_med <- readRDS(here("Dat/obs","dat_medicine.RDS"))
datlist_psy <- readRDS(here("Dat/obs","dat_psychology.RDS"))
datlist_psy2 <- readRDS(here("Dat/obs","dat_psychology2.RDS"))
# convert into dataframes
dat_eco <- bind_rows(datlist_eco, .id = "source")
dat_env <- bind_rows(datlist_env, .id = "source")
dat_med <- bind_rows(datlist_med, .id = "source")
dat_psy <- bind_rows(c(datlist_psy,datlist_psy2), .id = "source")
# convert the effect size from z to d
dat_eco <- dat_eco %>% mutate(mu_obs = z2d(z),
                              se_obs = se_z2se_d(se_z = z.se, z = z))

dat_env <- dat_env %>% mutate(mu_obs = z2d(z),
                              se_obs = se_z2se_d(se_z = z.se, z = z))

dat_med <- dat_med %>% mutate(mu_obs = z2d(z),
                              se_obs = se_z2se_d(se_z = z.se, z = z))

dat_psy <- dat_psy %>% mutate(mu_obs = z2d(z),
                              se_obs = se_z2se_d(se_z = z.se, z = z))
```

# Magnitude

In terms of effect size benchmark, the sign of the effect size does not add any value. Therefore, we use the folded distribution (https://en.wikipedia.org/wiki/Folded_normal_distribution) to calculate the folded mean (the magnitude) and folded standard error. 

```{r folded}
# folded mean and error
df <- df %>% mutate(folded_mu_adj = folded_es(mu_adj, var_adj),
                    folded_var_adj = folded_var(mu_adj, var_adj))

# subsets
df_eco <- df %>% filter(discipline == "economics")
df_env <- df %>% filter(discipline == "environmental")
df_med <- df %>% filter(discipline == "medicine")
df_psy <- df %>% filter(discipline == "psychology")


# a quick look at the difference between different strategies
df %>%
  group_by(discipline) %>%
  summarize(Q1_un = quantile(abs(mu_unadj), probs = 0.25),
            Q1_abs = quantile(abs(mu_adj), probs = 0.25),
            Q1_fold = quantile(folded_mu_adj, probs = 0.25),
            median_un = quantile(abs(mu_unadj), probs = 0.50),
            median_abs = quantile(abs(mu_adj), probs = 0.50),
            median_fold = quantile(folded_mu_adj, probs = 0.50),
            mean_un = mean(abs(mu_unadj)),
            mean_abs = mean(abs(mu_adj)),
            mean_fold = mean(folded_mu_adj),
            Q3_un = quantile(abs(mu_unadj), probs = 0.75),
            Q3_abs = quantile(abs(mu_adj), probs = 0.75),
            Q3_fold = quantile(folded_mu_adj, probs = 0.75)) %>% knitr::kable(caption = 'Comparing different strategies', digits = 3)



# reference benchmark - using middle 40% of distribution (30th to 70th percentile)
df %>%
  group_by(discipline) %>%
  summarize(p30_ori = quantile(abs(mu_unadj), probs = 0.3),
            p30_abs = quantile(abs(mu_adj), probs = 0.3),
            p30_fold = quantile(folded_mu_adj, probs = 0.3),
            p70_ori = quantile(abs(mu_unadj), probs = 0.7),
            p70_abs = quantile(abs(mu_adj), probs = 0.7),
            p70_fold = quantile(folded_mu_adj, probs = 0.7)) %>% knitr::kable(caption = 'Comparing different strategies', digits = 3)




summary(df_eco$folded_mu_adj)
summary(filter(df_eco, pval<=0.05)$folded_mu_adj)

summary(df_env$folded_mu_adj)
summary(filter(df_env, pval<=0.05)$folded_mu_adj)

summary(df_med$folded_mu_adj)
summary(filter(df_med, pval<=0.05)$folded_mu_adj)


summary(df_psy$folded_mu_adj)
summary(filter(df_psy, pval<=0.05)$folded_mu_adj)

```

# Effect size benchmark

We have three ways to get the effect size benchmark.

## (1) Empirical density

The first way is to use the percentile of the empirical density of meta-analytic level effect size estimates.

### Visualization

```{r}

#*************************************************#
#*--------------------economic-------------------#
#*************************************************#

# individual vs. meta-analytic
# prepare data
dat_long_eco <- data.frame(Approach = c(rep("Individual level", nrow(dat_eco)), rep("Meta-analytic", nrow(df_eco))),
                  Estimate = c(dat_eco$mu_obs, df_eco$mu_adj))


# plot
dat_long$Approach <- as.factor(dat_long$Approach)
dat_long$Approach <- factor(dat_long$Approach, levels = c("Individual level", "Meta-analytic"))

# https://rpkgs.datanovia.com/ggpubr/reference/ggdensity.html
p.den_eco <- ggdensity(dat_long, x = "Estimate",
   add = "none", rug = TRUE, alpha = 0.4,
   color = "Approach", fill = "Approach",
   palette = c("#5867AF", "#F1AEA7")) + 
  labs(title = "Economics", x = "Effect size", y = "Density", fill = "", color = "") +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank()) + 
  xlim(-3,3)




# original vs. adjusted
# prepare data
dat <- select(df_eco, id, mu_unadj, mu_adj, folded_mu_adj)
dat_long_eco2 <- dat %>%
  pivot_longer(cols = c(mu_unadj, mu_adj), 
               names_to = "Approach", 
               values_to = "Estimate") %>%
  mutate(Approach = dplyr::recode(Approach, 
                       mu_unadj = "Unadjusted meta-analytic", 
                       mu_adj = "Adjusted meta-analytic"))
# plot
dat_long$Approach <- as.factor(dat_long$Approach)
dat_long$Approach <- factor(dat_long$Approach, levels = c("Unadjusted meta-analytic", "Adjusted meta-analytic"))

# https://rpkgs.datanovia.com/ggpubr/reference/ggdensity.html
p.den_eco2 <- ggdensity(dat_long, x = "Estimate",
   add = "none", rug = TRUE, alpha = 0.4,
   color = "Approach", fill = "Approach",
   palette = c("#FCB462", "#7BC4C5")) + 
  labs(title = "A Economics", x = "Effect size", y = "Density", fill = "", color = "") +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())


# absolute vs. folded
dat_long_eco3 <- dat %>%
  pivot_longer(cols = c(mu_adj, folded_mu_adj), 
               names_to = "Approach", 
               values_to = "Estimate") %>%
  mutate(Approach = dplyr::recode(Approach, 
                       mu_adj = "Absolute meta-analytic", 
                       folded_mu_adj = "Folded meta-analytic"))
# plot
dat_long$Approach <- as.factor(dat_long$Approach)
dat_long$Approach <- factor(dat_long$Approach, levels = c("Absolute meta-analytic", "Folded meta-analytic"))
dat_long$Estimate <- abs(dat_long$Estimate)

p.den_eco3 <- ggdensity(dat_long, x = "Estimate",
   add = "none", rug = TRUE, alpha = 0.4,
   color = "Approach", fill = "Approach",
   palette = c("#8CA5EA", "#E3738B")) +
  labs(title = "B Economics", x = "Effect size", y = "Density", fill = "", color = "") +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())


png(filename = "Fig.density_eco.png", width = 10, height = 6, units = "in", type = "windows", res = 400)
# p.den_eco / p.den_eco2 / p.den_eco3
p.den_eco2 + p.den_eco3 + plot_layout(tag_level = 'new') +  plot_annotation(tag_levels = list(c(' ', " "))) & theme(plot.tag = element_text(size = 16, face = "bold")) 
dev.off()



#*************************************************#
#*--------------------environment-------------------#
#*************************************************#
# individual vs. meta-analytic
# prepare data
dat_long_env <- data.frame(Approach = c(rep("Individual level", nrow(dat_env)), rep("Meta-analytic", nrow(df_env))),
                  Estimate = c(dat_env$mu_obs, df_env$mu_adj))





# prepare data
dat <- select(df_env, id, mu_unadj, mu_adj, folded_mu_adj)

# original vs. adjusted
dat_long_env2 <- dat %>%
  pivot_longer(cols = c(mu_unadj, mu_adj), 
               names_to = "Approach", 
               values_to = "Estimate") %>%
  mutate(Approach = dplyr::recode(Approach, 
                       mu_unadj = "Original", 
                       mu_adj = "Adjusted"))
# plot
dat_long$Approach <- as.factor(dat_long$Approach)
dat_long$Approach <- factor(dat_long$Approach, levels = c("Original", "Adjusted"))

p.den_env <- ggdensity(dat_long, x = "Estimate",
   add = "none", rug = TRUE, alpha = 0.4,
   color = "Approach", fill = "Approach",
   palette = c("#FCB462", "#7BC4C5")) + 
  labs(title = "Environment", x = "Meta-analytic effect size estimate", y = "Density", fill = "", color = "") +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())


# absolute vs. folded
dat_long_env3 <- dat %>%
  pivot_longer(cols = c(mu_adj, folded_mu_adj), 
               names_to = "Approach", 
               values_to = "Estimate") %>%
  mutate(Approach = dplyr::recode(Approach, 
                       mu_adj = "Absolute", 
                       folded_mu_adj = "Folded"))
# plot
dat_long$Approach <- as.factor(dat_long$Approach)
dat_long$Approach <- factor(dat_long$Approach, levels = c("Absolute", "Folded"))
dat_long$Estimate <- abs(dat_long$Estimate)

p.den_env2 <- ggdensity(dat_long, x = "Estimate",
   add = "none", rug = TRUE, alpha = 0.4,
   color = "Approach", fill = "Approach",
   palette = c("#8CA5EA", "#E3738B")) +
  labs(title = "Environment", x = "Meta-analytic effect size estimate", y = "Density", fill = "", color = "") +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())


png(filename = "Fig.density_env.png", width = 10, height = 6, units = "in", type = "windows", res = 400)
p.den_env | p.den_env2
dev.off()


#*************************************************#
#*--------------------psychology-------------------#
#*************************************************#
# individual vs. meta-analytic
# prepare data
dat_long_psy <- data.frame(Approach = c(rep("Individual level", nrow(dat_psy)), rep("Meta-analytic", nrow(df_psy))),
                           Estimate = c(dat_psy$mu_obs, df_psy$mu_adj))

# prepare data
dat <- select(df_psy, id, mu_unadj, mu_adj, folded_mu_adj)

# original vs. adjusted
dat_long_psy2 <- dat %>%
  pivot_longer(cols = c(mu_unadj, mu_adj), 
               names_to = "Approach", 
               values_to = "Estimate") %>%
  mutate(Approach = dplyr::recode(Approach, 
                       mu_unadj = "Original", 
                       mu_adj = "Adjusted"))
# plot
dat_long$Approach <- as.factor(dat_long$Approach)
dat_long$Approach <- factor(dat_long$Approach, levels = c("Original", "Adjusted"))

p.den_psy <- ggdensity(dat_long, x = "Estimate",
   add = "none", rug = TRUE, alpha = 0.4,
   color = "Approach", fill = "Approach",
   palette = c("#FCB462", "#7BC4C5")) + 
  labs(title = "Psychology", x = "Meta-analytic effect size estimate", y = "Density", fill = "", color = "") +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())


# absolute vs. folded
dat_long_psy3 <- dat %>%
  pivot_longer(cols = c(mu_adj, folded_mu_adj), 
               names_to = "Approach", 
               values_to = "Estimate") %>%
  mutate(Approach = dplyr::recode(Approach, 
                       mu_adj = "Absolute", 
                       folded_mu_adj = "Folded"))
# plot
dat_long$Approach <- as.factor(dat_long$Approach)
dat_long$Approach <- factor(dat_long$Approach, levels = c("Absolute", "Folded"))
dat_long$Estimate <- abs(dat_long$Estimate)

p.den_psy2 <- ggdensity(dat_long, x = "Estimate",
   add = "none", rug = TRUE, alpha = 0.4,
   color = "Approach", fill = "Approach",
   palette = c("#8CA5EA", "#E3738B")) +
  labs(title = "Psychology", x = "Meta-analytic effect size estimate", y = "Density", fill = "", color = "") +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())


png(filename = "Fig.density_psy.png", width = 10, height = 6, units = "in", type = "windows", res = 400)
p.den_psy | p.den_psy2
dev.off()

#*************************************************#
#*--------------------medicine-------------------#
#*************************************************#

# individual vs. meta-analytic
# prepare data
dat_long_med <- data.frame(Approach = c(rep("Individual level", nrow(dat_med)), rep("Meta-analytic", nrow(df_med))),
                           Estimate = c(dat_med$mu_obs, df_med$mu_adj))

# prepare data
dat <- select(df_med, id, mu_unadj, mu_adj, folded_mu_adj)

# original vs. adjusted
dat_long_med2 <- dat %>%
  pivot_longer(cols = c(mu_unadj, mu_adj), 
               names_to = "Approach", 
               values_to = "Estimate") %>%
  mutate(Approach = dplyr::recode(Approach, 
                       mu_unadj = "Original", 
                       mu_adj = "Adjusted"))
# plot
dat_long$Approach <- as.factor(dat_long$Approach)
dat_long$Approach <- factor(dat_long$Approach, levels = c("Original", "Adjusted"))

p.den_med <- ggdensity(dat_long, x = "Estimate",
   add = "none", rug = TRUE, alpha = 0.4,
   color = "Approach", fill = "Approach",
   palette = c("#FCB462", "#7BC4C5")) + 
  labs(title = "Medicine", x = "Meta-analytic effect size estimate", y = "Density", fill = "", color = "") +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())


# absolute vs. folded
dat_long_med3 <- dat %>%
  pivot_longer(cols = c(mu_adj, folded_mu_adj), 
               names_to = "Approach", 
               values_to = "Estimate") %>%
  mutate(Approach = dplyr::recode(Approach, 
                       mu_adj = "Absolute", 
                       folded_mu_adj = "Folded"))
# plot
dat_long$Approach <- as.factor(dat_long$Approach)
dat_long$Approach <- factor(dat_long$Approach, levels = c("Absolute", "Folded"))
dat_long$Estimate <- abs(dat_long$Estimate)

p.den_med2 <- ggdensity(dat_long, x = "Estimate",
   add = "none", rug = TRUE, alpha = 0.4,
   color = "Approach", fill = "Approach",
   palette = c("#8CA5EA", "#E3738B")) +
  labs(title = "Medicine", x = "Meta-analytic effect size estimate", y = "Density", fill = "", color = "") +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())

png(filename = "Fig.density_psy.png", width = 10, height = 6, units = "in", type = "windows", res = 400)
p.den_med | p.den_med2
dev.off()

# assembly
png(filename = "Fig.density.png", width = 12, height = 12, units = "in", type = "windows", res = 400)
  p.den_psy + p.den_psy2 +
  p.den_med + p.den_med2 +
  p.den_eco + p.den_eco2 +
  p.den_env + p.den_env2 +
  plot_layout(nrow = 4, ncol = 2) + plot_annotation(tag_levels = list(c('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'))) & theme(plot.tag = element_text(size = 18, face = "bold"))
dev.off()
```


For economics:

```{r empirical density}
quantile(df_eco$folded_mu_adj, probs = c(.25, .5, .75))
```

For environment:

```{r}
quantile(df_env$folded_mu_adj, probs = c(.25, .5, .75))
```

For psychology:

```{r}
quantile(df_psy$folded_mu_adj, probs = c(.25, .5, .75))
```

For medicine:

```{r empirical Bayes}
quantile(df_med$folded_mu_adj, probs = c(.25, .5, .75))
```



## (2) Empirical Bayes

The second way is to calculate the empirical Bayes (or best linear unbiased prediction) corresponding to each meta-analytic level effect size and then use percentile of the empirical density of the empirical Bayes. By empirical Bayes, I mean the meta-analytic specific true effect size after borrowing strength (partial pooling in the context of multilevel model).

To do so, we: (1) fit a random-effect model to account for the second-order sampling error, (2) calculate the Best linear unbiased prediction.

First let's get the empirical Bayes for each discipline.

```{r}
#---------------------------------------------------#
#empirical Bayes
#---------------------------------------------------#

#------------------economics------------------#
res_eco <- meta(y = folded_mu_adj, v = folded_var_adj, data = df_eco)
df_eco <- df_eco %>% mutate(lamda = coef(res_eco)[2] / (coef(res_eco)[2] + df_eco$folded_var_adj),
                            blup = lamda * df_eco$folded_mu_adj + (1 - lamda) * coef(res_eco)[1])

# manual validation
res_eco2 <- rma(yi = folded_mu_adj, vi= folded_var_adj, method = "REML", data = df_eco)
# get Empirical Bayes
blup_eco <- blup(res_eco2) %>% as.data.frame()
# manual computation of blup
blup_eco <- blup_eco %>% mutate(lamda = res_eco2$tau2 / (res_eco2$tau2 + res_eco2$vi),
                                pred2 = lamda * res_eco2$yi + (1 - lamda) * res_eco2$beta[1])

df_eco2 <- df_eco %>% mutate(blup2 = blup_eco$pred,
                             blup3 = blup_eco$pred2)


# example data from metadata
dat <- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
res <- rma(yi, vi, data=dat)
blup_res <- blup(res) %>% as.data.frame()
blup_res <- blup_res %>% mutate(lamda = res$tau2 / (res$tau2 + res$vi),
                                pred2 = lamda * res$yi + (1 - lamda) * res$beta[1])

#------------------environment------------------#
res_env <- meta(y = folded_mu_adj, v = folded_var_adj, data = df_env)
df_env <- df_env %>% mutate(lamda = coef(res_env)[2] / (coef(res_env)[2] + df_env$folded_var_adj),
                            blup = lamda * df_env$folded_mu_adj + (1 - lamda) * coef(res_env)[1])


#------------------psychology------------------#
res_psy <- meta(y = folded_mu_adj, v = folded_var_adj, data = df_psy)
df_psy <- df_psy %>% mutate(lamda = coef(res_psy)[2] / (coef(res_psy)[2] + df_psy$folded_var_adj),
                            blup = lamda * df_psy$folded_mu_adj + (1 - lamda) * coef(res_psy)[1])



#------------------medicine------------------#
res_med <- meta(y = folded_mu_adj, v = folded_var_adj, data = df_med)
df_med <- df_med %>% mutate(lamda = coef(res_med)[2] / (coef(res_med)[2] + df_med$folded_var_adj),
                            blup = lamda * df_med$folded_mu_adj + (1 - lamda) * coef(res_med)[1])

# res_med1 <- readRDS(here("Dat","res_med1.rds"))
#saveRDS(res_med1, here("es_benchmark/es_benchmark_git/Dat","res_med1.rds"))
```

Then, we calculate the percentiles of the empirical Bayes.

For economics:

```{r}
quantile(df_eco$blup, probs = c(0.25, 0.50, 0.75)) %>% round(3)
```

For environment:

```{r}
quantile(df_env$blup, probs = c(0.25, 0.50, 0.75)) %>% round(3)
```

For psychology:

```{r}
quantile(df_psy$blup, probs = c(0.25, 0.50, 0.75)) %>% round(3)
```

For medicine:

```{r}
quantile(df_env$blup, probs = c(0.25, 0.50, 0.75)) %>% round(3)
```


### original vs. empirical Bates

```{r}
# Mann-Whitney U tests
## economics
wilcox.test(filter(df_eco, folded_mu_adj <= quantile(folded_mu_adj, 0.25))$folded_mu_adj, filter(df_eco, blup <= quantile(blup, 0.25))$blup)
wilcox.test(df_eco$folded_mu_adj, df_eco$blup)
wilcox.test(filter(df_eco, folded_mu_adj <= quantile(folded_mu_adj, 0.75))$folded_mu_adj, filter(df_eco, blup <= quantile(blup, 0.75))$blup)

## environment
wilcox.test(filter(df_env, folded_mu_adj <= quantile(folded_mu_adj, 0.25))$folded_mu_adj, filter(df_env, blup <= quantile(blup, 0.25))$blup)
wilcox.test(df_env$folded_mu_adj, df_env$blup)
wilcox.test(filter(df_env, folded_mu_adj <= quantile(folded_mu_adj, 0.75))$folded_mu_adj, filter(df_env, blup <= quantile(blup, 0.75))$blup)

## medicine
wilcox.test(filter(df_med, folded_mu_adj <= quantile(folded_mu_adj, 0.25))$folded_mu_adj, filter(df_med, blup <= quantile(blup, 0.25))$blup)
wilcox.test(df_med$folded_mu_adj, df_med$blup)
wilcox.test(filter(df_med, folded_mu_adj <= quantile(folded_mu_adj, 0.75))$folded_mu_adj, filter(df_med, blup <= quantile(blup, 0.75))$blup)

## psychology
wilcox.test(filter(df_psy, folded_mu_adj <= quantile(folded_mu_adj, 0.25))$folded_mu_adj, filter(df_psy, blup <= quantile(blup, 0.25))$blup)
wilcox.test(df_psy$folded_mu_adj, df_psy$blup)
wilcox.test(filter(df_psy, folded_mu_adj <= quantile(folded_mu_adj, 0.75))$folded_mu_adj, filter(df_psy, blup <= quantile(blup, 0.75))$blup)
```




## (3) Posterior distribution

The third way is to estimate the posterior distribution (empirical Bayes estimates) and get the percentiles. This should be affect by the prior. I am not sure which priors we would use. I think you will know more than me. So I leave this section to you.


# Equivalence test

I want to have a package to help users to automate the interpretation. My idea is to use the equivalence test to compare whether a given meta-analytic mean is equivalent to a threshold (benchmark) derived from the above three ways.

For example, assume we use 25-th, 50-th, and 75-th percentiles of economic effect sizes as the small, medium and large magnitude. Now, we conduct a meta-analysis and we get a point estimate of 0.18 and standard error 0.1. We want to know whether this point effect is equivalent to the medium threshold in the field of economics.


This can be done by:

```{r equivalence}
library("TOSTER")
TOSTmeta(ES = 0.3, se = 0.1, low_eqbound_d = quantile(df_env$folded_mu_adj, probs = c(.25, .5, .75))[1], high_eqbound_d = quantile(df_env$folded_mu_adj, probs = c(.25, .5, .75))[2], alpha = 0.05)


stat_error(0.3, 0.12)

stat_error <- function(mu, s, alpha=.05, df=Inf, n.sims=10000){
A <- mu
z <- qt(1-alpha/2, df) 
p.hi <- 1 - pt(z-A/s, df) 
p.lo <- pt(-z-A/s, df)  
power <- p.hi + p.lo
typeS <- p.lo/power
estimate <- A + s*rt(n.sims,df)
significant <- abs(estimate) > s*z
exaggeration <- mean(abs(estimate)[significant])/A
return(list(power=power, typeS=typeS, exaggeration=exaggeration))
}
```

We see that although the magnitude of the effect size (0.25) is larger than the 75-th percentile (0.2094795), it's magnitude is equivalent to the 50-th percentile (0.12798826), if considering the uncertainty. 





# Kolmogorov-Smirnov test

https://stats.stackexchange.com/questions/52305/comparison-of-2-distributions/52329#52329

implementation:
https://stat.ethz.ch/R-manual/R-patched/library/stats/html/ks.test.html

good explanations:
https://towardsdatascience.com/how-to-compare-two-distributions-in-practice-8c676904a285

https://stats.stackexchange.com/questions/4/assessing-the-significance-of-differences-in-distributions

https://stats.stackexchange.com/questions/9311/kullback-leibler-vs-kolmogorov-smirnov-distance

pvalue of the KS test: https://stats.stackexchange.com/questions/149595/ks-test-how-is-the-p-value-calculated


Bootstrapping based on the Kolmogorov-Smirnov test
https://cran.r-project.org/web/packages/kldtools/kldtools.pdf

```{r}
library(kldtools)
# economics
load(file = here("Dat", "ks_eco.rda"))
load(file = here("Dat", "ks_eco2.rda"))
load(file = here("Dat", "ks_eco3.rda"))
load(file = here("Dat", "ks_eco4.rda"))
# save to save time
ks_eco <- ksboot(dat_eco$obs, df_eco$mu_unadj)
ks_eco2 <- ksboot(df_eco$mu_unadj, df_eco$mu_adj)
ks_eco3 <- ksboot(abs(df_eco$mu_adj), df_eco$folded_mu_adj)
ks_eco4 <- ksboot(df_eco$folded_mu_adj, df_eco$blup)

save(ks_eco, file = here("Dat", "ks_eco.rda"))
save(ks_eco2, file = here("Dat", "ks_eco2.rda"))
save(ks_eco3, file = here("Dat", "ks_eco3.rda"))
save(ks_eco4, file = here("Dat", "ks_eco4.rda"))

## sensitivity
library(twosamples)
set.seed(2024)
cvm_eco <- cvm_test(dat_eco$mu_obs, df_eco$mu_unadj, nboots = 10000) # https://search.r-project.org/CRAN/refmans/twosamples/html/cvm_test.html
# https://codowd.com/public/DTS.pdf
cvm_eco2 <- cvm_test(df_eco$mu_unadj, df_eco$mu_adj, nboots = 10000)
cvm_eco3 <- cvm_test(abs(df_eco$mu_adj), df_eco$folded_mu_adj, nboots = 10000)
cvm_eco4 <- wass_test(df_eco$folded_mu_adj, df_eco$blup, nboots = 10000)

# environment
load(file = here("Dat", "ks_env.rda"))
load(file = here("Dat", "ks_env2.rda"))
load(file = here("Dat", "ks_env3.rda"))
load(file = here("Dat", "ks_env4.rda"))
# save to save time
ks_env <- ksboot(dat_env$z, df_env$mu_unadj)
ks_env2 <- ksboot(df_env$mu_unadj, df_env$mu_adj)
ks_env3 <- ksboot(abs(df_env$mu_adj), df_env$folded_mu_adj)
ks_env4 <- ksboot(df_env$blup, df_env$folded_mu_adj)

save(ks_env, file = here("Dat", "ks_env.rda"))
save(ks_env2, file = here("Dat", "ks_env2.rda"))
save(ks_env3, file = here("Dat", "ks_env3.rda"))
save(ks_env4, file = here("Dat", "ks_env4.rda"))


# psychology
load(file = here("Dat", "ks_psy.rda"))
load(file = here("Dat", "ks_psy2.rda"))
load(file = here("Dat", "ks_psy3.rda"))
load(file = here("Dat", "ks_psy4.rda"))
# save to save time
ks_psy <- ksboot(dat_psy$z, df_psy$mu_unadj)
ks_psy2 <- ksboot(df_psy$mu_unadj, df_psy$mu_adj)
ks_psy3 <- ksboot(abs(df_psy$mu_adj), df_psy$folded_mu_adj)
ks_psy4 <- ksboot(df_psy$blup, df_psy$folded_mu_adj)

save(ks_psy, file = here("Dat", "ks_psy.rda"))
save(ks_psy2, file = here("Dat", "ks_psy2.rda"))
save(ks_psy3, file = here("Dat", "ks_psy3.rda"))
save(ks_psy4, file = here("Dat", "ks_psy4.rda"))

# medicine
load(file = here("Dat", "ks_med.rda"))
load(file = here("Dat", "ks_med2.rda"))
load(file = here("Dat", "ks_med3.rda"))
load(file = here("Dat", "ks_med4.rda"))

# save to save time
ks_med <- ksboot(dat_med$z, df_med$mu_unadj)
ks_med2 <- ksboot(df_med$mu_unadj, df_med$mu_adj)
ks_med3 <- ksboot(abs(df_med$mu_adj), df_med$folded_mu_adj)
ks_med4 <- ksboot(df_med$blup, df_med$folded_mu_adj)

save(ks_med, file = here("Dat", "ks_med.rda"))
save(ks_med2, file = here("Dat", "ks_med2.rda"))
save(ks_med3, file = here("Dat", "ks_med3.rda"))
save(ks_med4, file = here("Dat", "ks_med4.rda"))


```

# Central tendence and dispersion

```{r}
# economics
# Mann-Whitney U Test (Wilcoxon signed-rank test) - non-parametric test compares the medians of two independent groups
wilcox_eco <- wilcox.test(dat_eco$mu_obs, df_eco$mu_unadj)
wilcox_eco2 <- wilcox.test(df_eco$mu_unadj, df_eco$mu_adj, paired = TRUE)
wilcox_eco3 <- wilcox.test(abs(df_eco$mu_adj), df_eco$folded_mu_adj, paired = TRUE)
wilcox_eco4 <- wilcox.test(df_eco$folded_mu_adj, df_eco$blup, paired = TRUE)

# F-test to compare the variances of two groups # https://en.wikipedia.org/wiki/F-test_of_equality_of_variances
Ftest_eco <- var.test(dat_eco$mu_obs, df_eco$mu_unadj)
Ftest_eco2 <- var.test(df_eco$mu_unadj, df_eco$mu_adj)
Ftest_eco3 <- var.test(abs(df_eco$mu_adj), df_eco$folded_mu_adj)
Ftest_eco4 <- var.test(df_eco$folded_mu_adj, df_eco$blup)

# Fligner-Killeen test to compare variances - non-parametric comparison of variances
fligner_eco <- fligner.test(Estimate ~ as.factor(Approach), data = dat_long_eco)


# environment
## central
wilcox_env <- wilcox.test(dat_env$mu_obs, df_env$mu_unadj)
wilcox_env2 <- wilcox.test(df_env$mu_unadj, df_env$mu_adj, paired = TRUE)
wilcox_env3 <- wilcox.test(abs(df_env$mu_adj), df_env$folded_mu_adj, paired = TRUE)
wilcox_env4 <- wilcox.test(df_env$folded_mu_adj, df_env$blup, paired = TRUE)

# dispersion
Ftest_env <- var.test(dat_env$mu_obs, df_env$mu_unadj)
Ftest_env2 <- var.test(df_env$mu_unadj, df_env$mu_adj)
Ftest_env3 <- var.test(abs(df_env$mu_adj), df_env$folded_mu_adj)
Ftest_env4 <- var.test(df_env$folded_mu_adj, df_env$blup)
fligner_env <- fligner.test(Estimate ~ as.factor(Approach), data = dat_long_env)

# medicine
## central
wilcox_med <- wilcox.test(dat_med$mu_obs, df_med$mu_unadj)
wilcox_med2 <- wilcox.test(df_med$mu_unadj, df_med$mu_adj, paired = TRUE)
wilcox_med3 <- wilcox.test(abs(df_med$mu_adj), df_med$folded_mu_adj, paired = TRUE)
wilcox_med4 <- wilcox.test(df_med$folded_mu_adj, df_med$blup, paired = TRUE)

# dispersion
Ftest_med <- var.test(dat_med$mu_obs, df_med$mu_unadj)
Ftest_med2 <- var.test(df_med$mu_unadj, df_med$mu_adj)
Ftest_med3 <- var.test(abs(df_med$mu_adj), df_med$folded_mu_adj)
Ftest_med4 <- var.test(df_med$folded_mu_adj, df_med$blup)
fligner_med <- fligner.test(Estimate ~ as.factor(Approach), data = dat_long_med)

# psychology
## central
wilcox_psy <- wilcox.test(dat_psy$mu_obs, df_psy$mu_unadj)
wilcox_psy2 <- wilcox.test(df_psy$mu_unadj, df_psy$mu_adj, paired = TRUE)
wilcox_psy3 <- wilcox.test(abs(df_psy$mu_adj), df_psy$folded_mu_adj, paired = TRUE)
wilcox_psy4 <- wilcox.test(df_psy$folded_mu_adj, df_psy$blup, paired = TRUE)

# dispersion
Ftest_psy <- var.test(dat_psy$mu_obs, df_psy$mu_unadj)
Ftest_psy2 <- var.test(df_psy$mu_unadj, df_psy$mu_adj)
Ftest_psy3 <- var.test(abs(df_psy$mu_adj), df_psy$folded_mu_adj)
Ftest_psy4 <- var.test(df_psy$folded_mu_adj, df_psy$blup)
fligner_psy <- fligner.test(Estimate ~ as.factor(Approach), data = dat_long_psy)

```


# boot
```{r}

library(boot)
# calculate mode
mode_func <- function(data, indices) {
  d <- data[indices]
  return(LaplacesDemon::Modes(d)$modes[1])
}

mode_func2 <- function(data, indices) {
  d <- data[indices]
  return(LaplacesDemon::Modes(d)$modes[2]) # for the second mode
}

# calculate skewness
skewness_func <- function(data, indices) {
  d <- data[indices]
  return(moments::skewness(d))
}

# calculate kurtosis
kurtosis_func <- function(data, indices) {
  d <- data[indices]
  return(moments::kurtosis(d))
}

# economic
## bootstrapping for mode
boot_mode_eco <- boot(df_eco$folded_mu_adj, mode_func, R = 5000)
## confidence intervals for mode
boot_mode_eco_ci <- boot.ci(boot_mode_eco, type = "bca") 
## bootstrapping for skewness
boot_skewness_eco <- boot(df_eco$folded_mu_adj, skewness_func, R = 5000)
## confidence intervals for skewness
boot_skewness_eco_ci <- boot.ci(boot_skewness_eco, type = "bca")
## bootstrapping for kurtosis
boot_kurtosis_eco <- boot(df_eco$folded_mu_adj, kurtosis_func, R = 5000)
## confidence intervals for kurtosis
boot_kurtosis_eco_ci <- boot.ci(boot_kurtosis_eco, type = "bca")

load(file = here("Dat", "boot_mode_eco.rda"))
load(file = here("Dat", "boot_mode_eco_ci.rda"))
load(file = here("Dat", "boot_skewness_eco.rda"))
load(file = here("Dat", "boot_skewness_eco_ci.rda"))
load(file = here("Dat", "boot_kurtosis_eco.rda"))
load(file = here("Dat", "boot_kurtosis_eco_ci.rda"))

save(boot_mode_eco, file = here("Dat", "boot_mode_eco.rda"))
save(boot_mode_eco_ci, file = here("Dat", "boot_mode_eco_ci.rda"))
save(boot_skewness_eco, file = here("Dat", "boot_skewness_eco.rda"))
save(boot_skewness_eco_ci, file = here("Dat", "boot_skewness_eco_ci.rda"))
save(boot_kurtosis_eco, file = here("Dat", "boot_kurtosis_eco.rda"))
save(boot_kurtosis_eco_ci, file = here("Dat", "boot_kurtosis_eco_ci.rda"))

# environment
## bootstrapping for mode
boot_mode_env <- boot(df_env$folded_mu_adj, mode_func, R = 10000)
## confidence intervals for mode
boot_mode_env_ci <- boot.ci(boot_mode_env, type = "bca") 

## bootstrapping for mode2
boot_mode_env2 <- boot(df_env$folded_mu_adj, mode_func2, R = 10000)
## confidence intervals for mode2
boot_mode_env_ci2 <- boot.ci(boot_mode_env2, type = "bca") 

## bootstrapping for skewness
boot_skewness_env <- boot(df_env$folded_mu_adj, skewness_func, R = 5000)
## confidence intervals for skewness
boot_skewness_env_ci <- boot.ci(boot_skewness_env, type = "bca")
## bootstrapping for kurtosis
boot_kurtosis_env <- boot(df_env$folded_mu_adj, kurtosis_func, R = 5000)
## confidence intervals for kurtosis
boot_kurtosis_env_ci <- boot.ci(boot_kurtosis_env, type = "bca")

load(file = here("Dat", "boot_mode_env.rda"))
load(file = here("Dat", "boot_mode_env_ci.rda"))
load(file = here("Dat", "boot_mode_env2.rda"))
load(file = here("Dat", "boot_mode_env_ci2.rda"))
load(file = here("Dat", "boot_skewness_env.rda"))
load(file = here("Dat", "boot_skewness_env_ci.rda"))
load(file = here("Dat", "boot_kurtosis_env.rda"))
load(file = here("Dat", "boot_kurtosis_env_ci.rda"))

save(boot_mode_env, file = here("Dat", "boot_mode_env.rda"))
save(boot_mode_env_ci, file = here("Dat", "boot_mode_env_ci.rda"))
save(boot_mode_env2, file = here("Dat", "boot_mode_env2.rda"))
save(boot_mode_env_ci2, file = here("Dat", "boot_mode_env_ci2.rda"))
save(boot_skewness_env, file = here("Dat", "boot_skewness_env.rda"))
save(boot_skewness_env_ci, file = here("Dat", "boot_skewness_env_ci.rda"))
save(boot_kurtosis_env, file = here("Dat", "boot_kurtosis_env.rda"))
save(boot_kurtosis_env_ci, file = here("Dat", "boot_kurtosis_env_ci.rda"))



# psychology
## bootstrapping for mode
boot_mode_psy <- boot(df_psy$folded_mu_adj, mode_func, R = 5000)
## confidence intervals for mode
boot_mode_psy_ci <- boot.ci(boot_mode_psy, type = "bca") 
## bootstrapping for skewness
boot_skewness_psy <- boot(df_psy$folded_mu_adj, skewness_func, R = 5000)
## confidence intervals for skewness
boot_skewness_psy_ci <- boot.ci(boot_skewness_psy, type = "bca")
## bootstrapping for kurtosis
boot_kurtosis_psy <- boot(df_psy$folded_mu_adj, kurtosis_func, R = 5000)
## confidence intervals for kurtosis
boot_kurtosis_psy_ci <- boot.ci(boot_kurtosis_psy, type = "bca")

load(file = here("Dat", "boot_mode_psy.rda"))
load(file = here("Dat", "boot_mode_psy_ci.rda"))
load(file = here("Dat", "boot_skewness_psy.rda"))
load(file = here("Dat", "boot_skewness_psy_ci.rda"))
load(file = here("Dat", "boot_kurtosis_psy.rda"))
load(file = here("Dat", "boot_kurtosis_psy_ci.rda"))

save(boot_mode_psy, file = here("Dat", "boot_mode_psy.rda"))
save(boot_mode_psy_ci, file = here("Dat", "boot_mode_psy_ci.rda"))
save(boot_skewness_psy, file = here("Dat", "boot_skewness_psy.rda"))
save(boot_skewness_psy_ci, file = here("Dat", "boot_skewness_psy_ci.rda"))
save(boot_kurtosis_psy, file = here("Dat", "boot_kurtosis_psy.rda"))
save(boot_kurtosis_psy_ci, file = here("Dat", "boot_kurtosis_psy_ci.rda"))


# medicine
## bootstrapping for mode
boot_mode_med <- boot(df_med$folded_mu_adj, mode_func, R = 5000)
## confidence intervals for mode
boot_mode_med_ci <- boot.ci(boot_mode_med, type = "bca") 
## bootstrapping for skewness
boot_skewness_med <- boot(df_med$folded_mu_adj, skewness_func, R = 5000)
## confidence intervals for skewness
boot_skewness_med_ci <- boot.ci(boot_skewness_med, type = "bca")
## bootstrapping for kurtosis
boot_kurtosis_med <- boot(df_med$folded_mu_adj, kurtosis_func, R = 5000)
## confidence intervals for kurtosis
boot_kurtosis_med_ci <- boot.ci(boot_kurtosis_med, type = "bca")


load(file = here("Dat", "boot_mode_med.rda"))
load(file = here("Dat", "boot_mode_med_ci.rda"))
load(file = here("Dat", "boot_skewness_med.rda"))
load(file = here("Dat", "boot_skewness_med_ci.rda"))
load(file = here("Dat", "boot_kurtosis_med.rda"))
load(file = here("Dat", "boot_kurtosis_med_ci.rda"))

save(boot_mode_med, file = here("Dat", "boot_mode_med.rda"))
save(boot_mode_med_ci, file = here("Dat", "boot_mode_med_ci.rda"))
save(boot_skewness_med, file = here("Dat", "boot_skewness_med.rda"))
save(boot_skewness_med_ci, file = here("Dat", "boot_skewness_med_ci.rda"))
save(boot_kurtosis_med, file = here("Dat", "boot_kurtosis_med.rda"))
save(boot_kurtosis_med_ci, file = here("Dat", "boot_kurtosis_med_ci.rda"))
```



# Distribution properities

bootstrap confidence intervals for

https://cran.r-project.org/web/packages/confintr/vignettes/confintr.html

https://cran.r-project.org/web/packages/DescTools/DescTools.pdf

```{r}
library(moments)
# economics
skewness(df_eco$folded_mu_adj)
kurtosis(df_eco$folded_mu_adj)
# environment
skewness(df_env$folded_mu_adj)
kurtosis(df_env$folded_mu_adj)
# medicine
skewness(df_med$folded_mu_adj)
kurtosis(df_med$folded_mu_adj)
# psychology
skewness(df_psy$folded_mu_adj)
kurtosis(df_psy$folded_mu_adj)


library(confintr)
ske_eco <- ci_skewness(dat_eco$z, R = length(dat_eco$z) * 1.5)


library(DescTools) 
# point estimate plus bootstrap CIs
Skew(df_med$folded_mu_adj, na.rm=TRUE, conf.level = 0.95)
Kurt(df_med$folded_mu_adj, na.rm=TRUE, conf.level = 0.95)

```



# Unimodal or multimodal

https://search.r-project.org/CRAN/refmans/performance/html/check_multimodal.html

https://universeofdatascience.com/how-to-determine-if-data-are-unimodal-or-multimodal-in-r/

# mixture model and likelihood ratio test
https://stats.stackexchange.com/questions/138223/how-to-test-if-my-distribution-is-multimodal
https://stats.stackexchange.com/questions/51062/test-for-bimodal-distribution

```{r}
library(performance)
check_multimodal(df_eco$folded_mu_adj)
check_multimodal(df_env$folded_mu_adj)
check_multimodal(df_med$folded_mu_adj)
check_multimodal(df_psy$folded_mu_adj)

library(LaplacesDemon)
# modes
LaplacesDemon::Modes(df_eco$folded_mu_adj)
LaplacesDemon::Modes(df_env$folded_mu_adj)
LaplacesDemon::Modes(df_med$folded_mu_adj)
LaplacesDemon::Modes(df_psy$folded_mu_adj)
# unimodal
LaplacesDemon::is.unimodal(df_eco$folded_mu_adj)
LaplacesDemon::is.unimodal(df_env$folded_mu_adj)
LaplacesDemon::is.unimodal(df_med$folded_mu_adj)
LaplacesDemon::is.unimodal(df_psy$folded_mu_adj)
# bimodal
LaplacesDemon::is.bimodal(df_eco$folded_mu_adj)
LaplacesDemon::is.bimodal(df_env$folded_mu_adj)
LaplacesDemon::is.bimodal(df_med$folded_mu_adj)
LaplacesDemon::is.bimodal(df_psy$folded_mu_adj)

#LaplacesDemon::is.trimodal(df_env$mu_adj)


library(diptest)
dip.test(df_eco$folded_mu_adj)
dip.test(df_env$folded_mu_adj)
dip.test(df_med$folded_mu_adj)
dip.test(df_psy$folded_mu_adj)


```


# Benchmarks

```{r}
# d
df %>%
  group_by(discipline) %>%
  summarize(Q1_fold = quantile(folded_mu_adj, probs = 0.25),
            median_fold = quantile(folded_mu_adj, probs = 0.50),
            Q3_fold = quantile(folded_mu_adj, probs = 0.75)) %>% dfround(3)

# r
df_r <- df %>% mutate(mu_adj_r = d2r(mu_adj), 
                      se_adj_r = se_d2se_r(se_adj, mu_adj)) %>%
  mutate(var_adj_r = se_adj_r^2,
         folded_mu_adj_r = folded_es(mu_adj_r, var_adj_r),
         folded_var_adj_r = folded_var(mu_adj_r, var_adj_r))
## thresholds
df_r %>%
  group_by(discipline) %>%
  summarize(Q1_fold = quantile(folded_mu_adj_r, probs = 0.25),
            median_fold = quantile(folded_mu_adj_r, probs = 0.50),
            Q3_fold = quantile(folded_mu_adj_r, probs = 0.75)) %>% dfround(3)

# eta squared
df_r %>%
  group_by(discipline) %>%
  summarize(Q1_fold = quantile(folded_mu_adj_r, probs = 0.25)^2,
            median_fold = quantile(folded_mu_adj_r, probs = 0.50)^2,
            Q3_fold = quantile(folded_mu_adj_r, probs = 0.75)^2) %>% dfround(3)

# Zr
df_Zr <- df_r %>% mutate(mu_adj_Zr = r2z(mu_adj_r), 
                      se_adj_Zr = se_r2se_z(se_adj_r, mu_adj_r)) %>%
  mutate(var_adj_Zr = se_adj_Zr^2,
         folded_mu_adj_Zr = folded_es(mu_adj_Zr, var_adj_Zr),
         folded_var_adj_Zr = folded_var(mu_adj_Zr, var_adj_Zr))

## thresholds
df_Zr %>%
  group_by(discipline) %>%
  summarize(Q1_fold = quantile(folded_mu_adj_Zr, probs = 0.25, na.rm = T),
            median_fold = quantile(folded_mu_adj_Zr, probs = 0.50,  na.rm = T),
            Q3_fold = quantile(folded_mu_adj_Zr, probs = 0.75,  na.rm = T)) %>% dfround(3)



# OR
df_OR <- df %>% mutate(mu_adj_OR = d2logOR(mu_adj), 
                      se_adj_OR = se_d2se_logOR(se_adj, mu_adj_OR)) %>%
  mutate(var_adj_OR = se_adj_OR^2,
         folded_mu_adj_OR = folded_es(mu_adj_OR, var_adj_OR),
         folded_var_adj_OR = folded_var(mu_adj_OR, var_adj_OR))
## thresholds
df_OR %>%
  group_by(discipline) %>%
  summarize(Q1_fold = quantile(folded_mu_adj_OR, probs = 0.25) %>% exp(),
            median_fold = quantile(folded_mu_adj_OR, probs = 0.50) %>% exp(),
            Q3_fold = quantile(folded_mu_adj_OR, probs = 0.75) %>% exp()) %>% dfround(3)

```



# Formula for folded mean and variance
https://search.r-project.org/CRAN/refmans/greybox/html/FNormal.html

https://en.wikipedia.org/wiki/Folded_normal_distribution

https://search.r-project.org/CRAN/refmans/VGAM/html/foldnormal.html

```{r}

```


# Validation:

```{r}
library(metafor)
dat <- dat.bangertdrowns2004
# fit a RE
res <- rma(yi, vi, data=dat)
# get shrinkage
dat$lamda <- res$tau2 / (res$tau2 + res$vi)
# get study-specific effect
dat$theta <- dat$lamda * res$yi + (1 - dat$lamda) * res$beta[1]
# use blup() to get theta 
theta <- blup(res)
# manual check
dat$theta == theta$pred

# fit the same RE as a location-scale model
res.ls <- rma(yi, vi, scale = ~ 1, data=dat)

```

